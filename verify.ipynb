{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from RzLinear import RzLinear \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checkout the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 128x128 chunk_size: 4 weight_size: 1000  tiled: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529,\n",
       "        530, 531,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,\n",
       "         68,  69,  70,  71, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 626, 627, 628, 629, 630, 631, 632, 633,\n",
       "        634, 635, 636, 637, 638, 639, 640, 641, 527, 528, 529, 530, 531, 532,\n",
       "        533, 534, 535, 536, 537, 538, 539, 540, 541, 542,  67,  68,  69,  70,\n",
       "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82, 352, 353,\n",
       "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
       "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
       "        267, 268], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_size = 1000\n",
    "input_dim = 128\n",
    "output_dim = 128\n",
    "chunk_size = 4\n",
    "\n",
    "#hashed_weight = nn.Parameter(torch.from_numpy(np.random.uniform(-1/np.sqrt(input_dim), 1/np.sqrt(input_dim), size=((weight_size,))).astype(np.float32)))\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n",
    "\n",
    "input_v = torch.eye(input_dim).to(\"cuda:0\")\n",
    "output_v = rzlinear(input_v)\n",
    "#print(output_v[:16,:16])\n",
    "#print(output_v[16:32,16:32])\n",
    "#print(output_v[16:96,:16])\n",
    "#plt.hist(np.array(output_v.detach().cpu().view(-1)))\n",
    "#np.max(np.array(output_v.detach().cpu().view(-1)))\n",
    "output_v[1,:].long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(255., device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(input_dim/16)):\n",
    "    for j in range(int(input_dim/16)):\n",
    "        x = output_v[i*16:(i+1)*16, j*16:(j+1)*16].reshape(-1)\n",
    "        print(x[-1] - x[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout the Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 1000x1000 chunk_size: 2 weight_size: 100000  tiled: True\n",
      "torch.Size([1000, 1000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "weight_size = 100000\n",
    "input_dim = 1000\n",
    "output_dim = 1000\n",
    "chunk_size = 2\n",
    "\n",
    "#hashed_weight = nn.Parameter(torch.from_numpy(np.random.uniform(-1/np.sqrt(input_dim), 1/np.sqrt(input_dim), size=((weight_size,))).astype(np.float32)))\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n",
    "\n",
    "input_v = torch.eye(input_dim).to(\"cuda:0\")\n",
    "output_v = rzlinear(input_v)\n",
    "print(output_v.shape)\n",
    "\n",
    "\n",
    "plt.hist(np.array(output_v.detach().cpu()).reshape(-1), bins = int(input_dim/10))\n",
    "plt.show()\n",
    "#output_v[1,:].long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkout the correctness of forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 1000x1000 chunk_size: 5 weight_size: 1000000  tiled: True\n"
     ]
    }
   ],
   "source": [
    "weight_size = 1000000\n",
    "input_dim = 1000\n",
    "output_dim = 1000\n",
    "chunk_size = 5\n",
    "\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n",
    "input_v = torch.eye(input_dim).to(\"cuda:0\")\n",
    "idx_matrix = rzlinear(input_v).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      1,      2,  ...,    997,    998,    999],\n",
       "        [   100,    101,    102,  ...,   1097,   1098,   1099],\n",
       "        [   200,    201,    202,  ...,   1197,   1198,   1199],\n",
       "        ...,\n",
       "        [ 99700,  99701,  99702,  ..., 100697, 100698, 100699],\n",
       "        [ 99800,  99801,  99802,  ..., 100797, 100798, 100799],\n",
       "        [ 99900,  99901,  99902,  ..., 100897, 100898, 100899]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_matrix.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205357,   80759397])\n",
      "RzLinear: d1xd2: 1000x1000 chunk_size: 5 weight_size: 1000000  tiled: True\n"
     ]
    }
   ],
   "source": [
    "hashed_weight = nn.Parameter(torch.from_numpy(np.random.uniform(-1,1, size=(weight_size,)).astype(np.float32)))\n",
    "rzlinear = RzLinear(input_dim, output_dim, chunk_size, hashed_weight).to(\"cuda:0\");\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_v = torch.rand((5,input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rzlinear(input_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = hashed_weight[idx_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = torch.matmul(input_v, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK\n"
     ]
    }
   ],
   "source": [
    "if torch.norm(out - ground_truth) == 0:\n",
    "    print(\"All OK\")\n",
    "else:\n",
    "    print(\"Issue in forward pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RzLinear import RzLinearFunction\n",
    "import torch\n",
    "from RzLinear import RzLinear \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2038074743,  634329019, 1825252241,  871205357,   80759397],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "TILED = True\n",
    "weight_size = 1000\n",
    "input_dim = 64\n",
    "output_dim = 64\n",
    "chunk_size = 1\n",
    "seed = 1024\n",
    "r = np.random.RandomState(seed)\n",
    "x = r.randint(0, 2038074743, (50,))\n",
    "x = x + 1*(x%2==0);\n",
    "random_numbers = torch.from_numpy(np.concatenate([np.array([2038074743]), x])).long().cuda(0) # set of 50 random numbers to use\n",
    "print(random_numbers[:5])\n",
    "hashed_weight = nn.Parameter(torch.from_numpy(np.arange(weight_size).astype(np.float32))).to(\"cuda:0\")\n",
    "input_v = torch.eye(input_dim).cuda(0)\n",
    "#hashed_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[259., 260., 261.,  ..., 594., 595., 596.],\n",
       "        [275., 276., 277.,  ..., 610., 611., 612.],\n",
       "        [291., 292., 293.,  ..., 626., 627., 628.],\n",
       "        ...,\n",
       "        [240., 241., 242.,  ..., 727., 728., 729.],\n",
       "        [256., 257., 258.,  ..., 743., 744., 745.],\n",
       "        [272., 273., 274.,  ..., 759., 760., 761.]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RzLinearFunction.forwardproxy(hashed_weight, input_v ,random_numbers, input_dim, output_dim, chunk_size, TILED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, tiled):\n",
    "    out = RzLinearFunction.forwardproxy(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size , tiled)\n",
    "    return out, torch.sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(1972224., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out, val = myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED )\n",
    "torch.cuda.synchronize()\n",
    "print(\"loss\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = out * 0 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4096., device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_grad, in_grad = RzLinearFunction.backwardproxy(grad, hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4096., device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(wt_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.001\n",
    "_, f0 = myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED)\n",
    "hwt_grad = torch.empty_like(hashed_weight, dtype=torch.float32)\n",
    "for i in range(len(hashed_weight)):\n",
    "#for i in [10]:\n",
    "    hwt = hashed_weight.clone()\n",
    "    hwt[i] += epsilon\n",
    "    _, fi = myFunc(hwt, input_v, random_numbers, input_dim, output_dim, chunk_size,TILED)\n",
    "    hwt_grad[i] = (fi - f0) / epsilon\n",
    "    #print(i,np.float(fi.cpu()),np.float(f0.cpu()), hwt_grad[i],  wt_grad[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error norm tensor(16079.9502, device='cuda:0')\n",
      "tensor([15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990,\n",
      "        15499.9990, 15999.9990, 16749.9980, 15999.9990, 16249.9990, 15999.9990,\n",
      "        16749.9980, 16749.9980, 16249.9990, 15999.9990, 16749.9980, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 14999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 16749.9980, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        15999.9990, 14999.9990, 14999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15499.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16249.9990, 15999.9990, 16749.9980, 16249.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 14999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 16249.9990, 16749.9980, 16249.9990, 16249.9990, 16749.9980,\n",
      "        16249.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        14999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 16749.9980,\n",
      "        16749.9980, 15999.9990, 15999.9990, 16749.9980, 16749.9980, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 16749.9980, 16749.9980, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 16749.9980, 16249.9990, 15999.9990,\n",
      "        16249.9990, 16749.9980, 15999.9990, 15499.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        16249.9990, 16749.9980, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 16749.9980, 16749.9980,\n",
      "        16749.9980, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 16249.9990, 16749.9980, 16249.9990,\n",
      "        15499.9990, 15499.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 16249.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 14999.9990, 14999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 14999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15499.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 16249.9990, 16749.9980, 15999.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 14999.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 16749.9980,\n",
      "        15999.9990, 16749.9980, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15499.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15499.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16249.9990, 16749.9980, 16249.9990, 16249.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        15499.9990, 15999.9990, 16249.9990, 16749.9980, 15999.9990, 16249.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15499.9990, 15999.9990, 16249.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990, 15499.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 15499.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990, 16249.9990,\n",
      "        16749.9980, 16749.9980, 16249.9990, 16749.9980, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        14999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16749.9980, 16749.9980, 15999.9990, 16749.9980, 16749.9980,\n",
      "        15999.9990, 16249.9990, 16749.9980, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 14999.9990, 15499.9990, 15999.9990, 15999.9990, 15499.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 15499.9990, 15999.9990, 16249.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 15999.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15499.9990, 15499.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16749.9980, 16249.9990, 16249.9990,\n",
      "        16749.9980, 16749.9980, 16249.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 14999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 16249.9990,\n",
      "        16249.9990, 15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16249.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        15999.9990, 15499.9990, 15999.9990, 16249.9990, 15499.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 16249.9990, 16249.9990, 16249.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15499.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 14999.9990,\n",
      "        15999.9990, 15999.9990, 14999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 16249.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15499.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 14999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        16749.9980, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 15999.9990,\n",
      "        14999.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990, 16249.9990,\n",
      "        16749.9980, 15999.9990, 15999.9990, 16749.9980, 16749.9980, 16249.9990,\n",
      "        15499.9990, 15999.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16749.9980, 16749.9980, 16249.9990,\n",
      "        16749.9980, 16249.9990, 15999.9990, 15999.9990, 15999.9990, 14999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 16749.9980, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990, 16249.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 16249.9990, 16749.9980, 16749.9980, 16749.9980,\n",
      "        16749.9980, 15999.9990, 16749.9980, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15499.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990, 16749.9980,\n",
      "        15999.9990, 16249.9990, 16249.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15499.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990, 16249.9990,\n",
      "        16249.9990, 16749.9980, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 16249.9990, 16749.9980, 16749.9980, 16249.9990,\n",
      "        16749.9980, 16749.9980, 16749.9980, 15999.9990, 16749.9980, 16749.9980,\n",
      "        16249.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 16249.9990, 15499.9990, 15999.9990, 15999.9990, 16749.9980,\n",
      "        16249.9990, 16249.9990, 16249.9990, 15999.9990, 15999.9990, 15999.9990,\n",
      "        16249.9990, 16249.9990, 15999.9990, 16249.9990, 15999.9990, 15999.9990,\n",
      "        15999.9990, 15999.9990, 15999.9990, 15999.9990], device='cuda:0')\n",
      "tensor([15351.2461, 15492.1709, 15581.8164, 15818.1152, 15156.3457, 15512.9531,\n",
      "        15365.6016, 15757.6953, 15227.3740, 15333.2002, 15288.6318, 15311.3955,\n",
      "        14924.3701, 15416.9648, 16067.4961, 15692.7871, 15869.9805, 15797.9131,\n",
      "        16018.2539, 15993.6416, 15867.8486, 15529.4023, 16096.5576, 15732.9199,\n",
      "        15420.8242, 15428.0596, 15745.6846, 15278.4404, 15475.2842, 15420.6797,\n",
      "        15623.2158, 15735.4502, 15432.0273, 15634.4844, 15753.6758, 15747.2402,\n",
      "        15493.9004, 15703.8330, 15512.0654, 15337.1309, 15017.2324, 14980.8574,\n",
      "        15382.6143, 15314.1641, 15212.1406, 15306.2676, 15706.4092, 15483.2568,\n",
      "        15214.8838, 15448.0635, 15807.4756, 15510.9961, 15471.8838, 15383.6504,\n",
      "        15743.9043, 15501.3750, 15057.2656, 15392.0439, 15585.1670, 15563.0049,\n",
      "        15138.5420, 15313.0176, 15317.5469, 15528.2686, 15119.4268, 15526.7881,\n",
      "        16130.8721, 15590.5713, 15434.3906, 15780.1914, 15856.9883, 15829.7744,\n",
      "        15700.1816, 15554.6201, 15980.2959, 15659.7803, 15436.7451, 15563.8535,\n",
      "        15710.5176, 15061.3584, 15441.8486, 15748.7705, 15821.6113, 15626.5811,\n",
      "        15453.5371, 15898.8027, 15830.1963, 15348.5889, 15396.1240, 15829.4678,\n",
      "        15547.4922, 14855.8135, 15145.0693, 15218.5723, 15755.3662, 15206.3037,\n",
      "        15465.9844, 15534.8018, 16110.7168, 15226.0088, 15465.8389, 15725.0439,\n",
      "        16037.6084, 15467.7549, 15794.4229, 15471.2793, 15973.8965, 15717.0020,\n",
      "        15201.9316, 15322.2520, 15736.0635, 15792.7061, 15202.1875, 15620.7744,\n",
      "        15521.8613, 15518.6279, 15011.5078, 15645.0957, 15793.2676, 15397.8555,\n",
      "        15194.5820, 15609.6982, 15487.2490, 15224.1094, 15580.1777, 15616.5186,\n",
      "        15846.9531, 15433.2539, 15404.7178, 15577.1650, 15656.8350, 15221.4316,\n",
      "        15287.5605, 15886.9775, 15852.1738, 15872.7002, 15709.7852, 16310.4453,\n",
      "        15925.7617, 15309.1250, 15224.5928, 15823.8154, 15414.0020, 14943.8750,\n",
      "        14963.2842, 15563.9434, 15690.6689, 15239.6650, 15479.2744, 15831.9854,\n",
      "        16221.0596, 15470.4668, 15573.0996, 16176.4053, 15909.6309, 15463.5244,\n",
      "        15768.9580, 15704.6113, 15811.6152, 15682.0596, 15374.0127, 15480.0596,\n",
      "        15714.5732, 15675.0586, 15433.8281, 15960.2100, 15443.0918, 15731.7227,\n",
      "        15484.5664, 15793.7451, 15505.5947, 15389.4482, 15267.4141, 15614.3359,\n",
      "        15316.5205, 15163.7207, 15615.5146, 15327.9385, 15530.8252, 15617.9414,\n",
      "        15884.9268, 15743.9463, 15673.7090, 15696.6406, 15888.1094, 16124.3359,\n",
      "        15890.8115, 16138.1543, 16098.4414, 16131.5977, 15540.4785, 15461.6982,\n",
      "        15465.9902, 16049.6553, 15204.8838, 15297.3311, 15366.5742, 15821.2920,\n",
      "        15478.3223, 15375.0400, 15770.6162, 16068.3906, 15993.2158, 15592.0361,\n",
      "        15879.5684, 16253.1172, 15551.4277, 15164.3242, 15733.9609, 15902.4727,\n",
      "        15693.7061, 15708.1729, 15713.2451, 15484.6309, 15926.5049, 15696.5547,\n",
      "        15870.1553, 16217.7764, 15743.8193, 15571.9961, 15573.9209, 15589.5361,\n",
      "        15458.6973, 15357.4209, 15468.8906, 15534.7842, 15479.4639, 14971.7500,\n",
      "        15591.9336, 15285.4082, 15581.4189, 15260.9697, 16099.5322, 15998.8203,\n",
      "        15930.6104, 15791.7354, 15885.7090, 15747.5762, 15655.2812, 15591.6143,\n",
      "        15753.6104, 15825.7734, 15317.5010, 15472.8154, 15600.4883, 15907.9893,\n",
      "        15318.6455, 15541.6758, 15633.6025, 15847.7568, 15343.3467, 15345.3691,\n",
      "        15668.4648, 15848.9590, 15652.3584, 15867.8203, 16163.6250, 15949.3789,\n",
      "        15294.6992, 15006.9238, 15784.3584, 15588.6006, 15557.1484, 15696.6523,\n",
      "        15827.2188, 15301.0283, 15864.3154, 15763.5547, 15818.1582, 15963.4307,\n",
      "        15744.2734, 15307.3984, 15514.2314, 15246.4570, 14931.5869, 15025.6162,\n",
      "        15160.1230, 15170.4697, 15182.5703, 14910.3594, 15504.2676, 15215.8145,\n",
      "        15472.7295, 15297.4717, 16240.2783, 15614.6758, 15770.0352, 15727.3184,\n",
      "        15716.2490, 15396.5537, 15535.5967, 15670.4531, 15677.7842, 15369.3418,\n",
      "        15265.5078, 15431.2832, 15775.5908, 15579.6260, 15463.1934, 15759.7275,\n",
      "        15828.1201, 15733.5908, 15575.3359, 15447.4463, 15650.4316, 15574.7295,\n",
      "        15241.1523, 15669.5693, 15809.5439, 15533.8350, 15464.7021, 15350.2324,\n",
      "        15682.7783, 15719.1914, 15740.8193, 15854.7461, 15823.9814, 15524.5195,\n",
      "        15599.3633, 15902.2881, 15849.2949, 15895.7637, 15621.9932, 15536.4287,\n",
      "        15390.4131, 15350.0830, 14715.1416, 15304.5684, 15231.1475, 15029.0459,\n",
      "        15005.2549, 15354.7471, 15677.1973, 15363.0811, 15577.0107, 15589.8867,\n",
      "        16398.6777, 15515.7666, 15448.1279, 15664.1006, 15708.1836, 15197.0781,\n",
      "        15679.9102, 15841.0029, 15816.9492, 15670.9150, 15649.6924, 15746.6865,\n",
      "        15994.2773, 15672.0479, 15662.7881, 15877.5391, 15737.9727, 15445.5078,\n",
      "        15787.6758, 15569.2275, 15635.7637, 15296.0195, 15435.3291, 15566.2646,\n",
      "        15289.8809, 14964.1719, 15427.1836, 15584.1680, 15462.5615, 15662.0068,\n",
      "        15743.5166, 16034.5234, 15562.9746, 15512.8916, 15539.8574, 16287.8066,\n",
      "        15802.7412, 15636.7432, 15734.1729, 16191.8965, 15574.2197, 15556.7725,\n",
      "        15535.1758, 16088.6504, 15515.1719, 15217.5498, 15367.9229, 15883.7793,\n",
      "        15830.6445, 15329.2627, 15502.2627, 15792.5332, 16008.9395, 15230.8203,\n",
      "        15074.3047, 15306.4717, 15284.2939, 14666.8193, 15249.5762, 15561.0166,\n",
      "        15523.9805, 15585.8564, 15565.1211, 15645.8301, 16195.6553, 15854.2324,\n",
      "        15536.4062, 16104.2021, 15987.4922, 15487.6543, 15605.2930, 15593.1973,\n",
      "        15555.3145, 15184.6113, 15325.6924, 15365.2236, 15621.6387, 15168.2256,\n",
      "        15707.4209, 15835.0645, 15852.4043, 15598.6016, 15870.6719, 15964.8125,\n",
      "        15471.6641, 15320.3789, 15554.6670, 16166.2188, 15476.1064, 15456.5039,\n",
      "        15653.1064, 16009.1982, 15181.4082, 15708.8359, 15764.2637, 16082.1729,\n",
      "        15532.0195, 15756.4395, 15591.6104, 15986.3848, 15648.2959, 15410.8945,\n",
      "        15460.1943, 15824.3682, 15573.9160, 15277.1611, 15340.2383, 15451.8877,\n",
      "        15256.6260, 14984.3066, 15579.9473, 15731.2871, 15527.9395, 15774.5977,\n",
      "        15742.3301, 15912.0977, 15958.6367, 15986.8271, 15849.0088, 16246.3350,\n",
      "        15800.9453, 15884.5449, 15761.1523, 15553.9717, 15363.3486, 15457.3418,\n",
      "        15534.3535, 15447.1260, 15638.3633, 15242.0566, 15777.7256, 15523.0146,\n",
      "        15575.7500, 15364.2012, 15985.2432, 15575.4229, 15378.9590, 15267.2998,\n",
      "        15517.6836, 15758.7852, 15268.4629, 15430.5811, 15686.8252, 15896.5020,\n",
      "        15155.0039, 15781.1094, 15915.8867, 16013.1943, 15516.0947, 15840.6660,\n",
      "        15755.4004, 16116.9268, 15530.0273, 15265.6025, 15494.2910, 15821.8896,\n",
      "        15319.6514, 15382.1133, 15555.8809, 15358.0254, 15055.5928, 15179.4385,\n",
      "        15354.8750, 15736.3848, 15354.9141, 15653.5273, 15824.2061, 15951.1924,\n",
      "        15586.3535, 15783.1270, 15559.6182, 16037.7734, 15409.5820, 15596.5586,\n",
      "        15595.6885, 15694.1807, 15210.7725, 15300.8379, 15695.8945, 15696.4219,\n",
      "        16037.6553, 15467.3281, 16040.2188, 15569.0918, 15555.1904, 15124.7812,\n",
      "        15910.2549, 15336.6455, 15289.8271, 15217.8965, 15498.0439, 15278.2100,\n",
      "        15410.8369, 15513.7266, 15712.5684, 15947.7393, 15739.2383, 15894.9922,\n",
      "        16234.7666, 16005.3252, 15826.0879, 15954.6982, 15828.2109, 15909.6367,\n",
      "        15389.9795, 15143.1396, 15305.7275, 15133.7119, 15120.9365, 15321.4492,\n",
      "        15521.3711, 15198.9619, 15391.4863, 15191.0967, 15536.3047, 15439.1465,\n",
      "        15452.8105, 15351.9473, 15777.3398, 15450.5830, 15324.8838, 15623.3867,\n",
      "        15562.5576, 15667.9521, 15610.6582, 16059.0127, 15763.3770, 15620.8799,\n",
      "        15278.1689, 15665.4238, 15571.1592, 15460.5586, 15887.3711, 15872.6387,\n",
      "        15979.7080, 15478.0186, 15503.5586, 15357.7080, 15800.1836, 15274.6377,\n",
      "        15068.6113, 15173.4756, 15336.5850, 15095.3682, 15307.2676, 15658.3828,\n",
      "        15737.2754, 16208.9863, 16091.2832, 15718.9658, 16256.3779, 16037.3965,\n",
      "        15607.6064, 15817.1963, 16024.1797, 15768.7109, 15716.8643, 15251.1055,\n",
      "        15528.3496, 15393.9092, 15701.6084, 15459.7578, 15874.9131, 15304.0439,\n",
      "        15820.8955, 15326.6367, 15766.7100, 15461.8809, 15676.0186, 15536.0293,\n",
      "        15773.5879, 15013.5674, 15162.6221, 15607.6299, 15340.5410, 15027.9219,\n",
      "        15534.0039, 16184.4355, 15547.3057, 15553.4512, 15627.2549, 15880.9639,\n",
      "        15508.5820, 15594.9287, 15671.9268, 15814.2773, 15763.9102, 15260.1318,\n",
      "        15264.4131, 15474.0459, 15577.8926, 15232.6074, 15348.0674, 15395.2217,\n",
      "        15241.8242, 15125.3936, 15246.9004, 15862.7607, 15711.9297, 15919.8252,\n",
      "        16045.6260, 15810.9746, 15896.2148, 15591.6670, 15407.9424, 15709.7295,\n",
      "        15642.7207, 15582.1592, 15684.2178, 15658.5781, 15417.4297, 15824.0684,\n",
      "        15924.0576, 15723.4473, 15951.9688, 15657.4023, 15614.0645, 15493.0430,\n",
      "        15477.0996, 15237.5098, 15570.7588, 15488.8867, 15518.6074, 15223.3262,\n",
      "        15290.3301, 15720.5371, 15537.8047, 15354.2148, 15639.1943, 16263.1982,\n",
      "        15622.0605, 15648.7285, 15727.2168, 15969.5059, 15615.6250, 15755.3018,\n",
      "        15536.9678, 15786.3691, 15787.8232, 15142.1133, 15374.9834, 15770.5557,\n",
      "        15777.0312, 15294.7275, 15605.9219, 15700.2158, 15771.7656, 15635.4580,\n",
      "        15541.6533, 16234.4473, 15849.8096, 15734.5957, 15558.3730, 15809.9814,\n",
      "        15555.0859, 15801.1514, 15481.6289, 16026.7373, 15855.5215, 15977.6670,\n",
      "        15852.2188, 15890.4043, 15372.6152, 16062.9697, 16008.0293, 15945.5977,\n",
      "        16074.8691, 16109.8838, 15903.5576, 15743.4775, 15560.7324, 15341.3535,\n",
      "        15574.2588, 15265.2920, 14976.3379, 14858.1494, 15487.1807, 15520.5234,\n",
      "        15332.3027, 15432.8496, 15925.4004, 16041.3535, 15606.3164, 15520.1982,\n",
      "        16027.4043, 15821.2168, 15702.5146, 15727.0117, 15921.9795, 15889.2578,\n",
      "        15823.6777, 15297.5322, 15800.2949, 15710.4795, 15826.0781, 15513.7617,\n",
      "        16107.9443, 15789.8926, 15932.5928, 15672.6943, 15766.3428, 16027.9014,\n",
      "        15455.7275, 15240.4082, 15450.0938, 15637.2275, 15156.9043, 15563.6201,\n",
      "        15671.8682, 16049.0820, 15558.6807, 15769.9697, 15730.9180, 15653.9150,\n",
      "        15357.8428, 15862.3936, 15960.4004, 15814.8721, 15757.3623, 15847.5400,\n",
      "        15655.5684, 15537.4492, 15556.8135, 15239.6533, 15617.4336, 15432.2383,\n",
      "        15121.8359, 15112.3311, 15570.5781, 15490.8408, 15223.7461, 15494.1318,\n",
      "        15716.1699, 15764.0469, 15481.5430, 15283.3477, 15749.3652, 15367.7422,\n",
      "        15265.2393, 15387.5176, 15729.3525, 15415.9668, 15542.6396, 15328.3896,\n",
      "        15701.1484, 15647.3799, 15773.8184, 15552.5820, 16399.8145, 15654.0195,\n",
      "        15828.4805, 15512.7666, 15665.0664, 15568.8594, 15209.0879, 14958.3223,\n",
      "        15516.7441, 15373.3818, 14920.1963, 15381.9375, 15539.5596, 15615.6855,\n",
      "        15568.9336, 15898.2354, 15770.2949, 15571.6387, 15623.1299, 15781.5947,\n",
      "        15959.0586, 15881.6055, 15787.3428, 15840.2363, 15770.2393, 15518.2051,\n",
      "        15499.7480, 15594.3594, 15839.1387, 15397.3799, 15160.9854, 15603.9980,\n",
      "        15660.2441, 15527.7363, 15208.4248, 15800.3633, 15597.3213, 15700.5186,\n",
      "        15360.9160, 15398.4258, 15460.3037, 15195.1006, 14779.6699, 15429.4072,\n",
      "        15474.1094, 15503.7354, 15715.7959, 15725.7520, 15622.4619, 15796.2627,\n",
      "        16070.2842, 15817.8877, 16329.7686, 15665.4512, 15871.4014, 15499.7217,\n",
      "        15561.0869, 15262.9307, 15479.0322, 15345.9922, 15754.8760, 15458.4453,\n",
      "        15042.6484, 15616.4775, 15652.0439, 15297.4268, 15295.4521, 15735.0430,\n",
      "        15752.2402, 15334.8105, 15397.4141, 15510.5752, 15873.0635, 15446.0010,\n",
      "        15526.6230, 15670.8311, 16004.4297, 15575.5586, 15398.3467, 15752.6289,\n",
      "        16007.2129, 15747.8535, 15562.2314, 15908.8516, 15960.9424, 15840.1709,\n",
      "        15275.4053, 15703.5791, 15856.1045, 16014.2471, 15553.4688, 15891.0518,\n",
      "        15861.7100, 15722.3643, 15039.7832, 15665.1006, 15505.8984, 15690.9492,\n",
      "        15886.3994, 15930.2754, 15572.7598, 16004.9854, 16128.1816, 15956.9912,\n",
      "        16019.1602, 15800.7783, 15696.1826, 15374.7100, 15460.6172, 14635.6094,\n",
      "        15279.0977, 15276.8467, 15360.0088, 15283.8887, 15439.7021, 15786.7188,\n",
      "        15440.4990, 15467.2734, 15588.0166, 16046.6836, 15661.4121, 15519.3779,\n",
      "        15661.2666, 15513.6084, 15546.3037, 15545.4502, 15754.5098, 15763.7314,\n",
      "        16020.9609, 15554.7207, 15775.8857, 16031.1357, 16125.0293, 15949.0801,\n",
      "        16094.2637, 15886.3184, 15983.4482, 15848.4316, 15216.1475, 15529.6914,\n",
      "        15503.2803, 15545.5049, 15213.9053, 15848.6904, 15578.8223, 15533.1084,\n",
      "        15153.6748, 15815.1982, 15773.0908, 15819.8887, 15902.0684, 16132.0264,\n",
      "        15488.7090, 15858.1924, 15865.8643, 15917.3867, 15671.8799, 15853.6406,\n",
      "        15805.5176, 15606.7715, 15630.3770, 15195.9375, 15630.0908, 15611.3350,\n",
      "        15436.0186, 15692.0010, 15768.8242, 16058.4229, 15507.3828, 15843.7080,\n",
      "        15704.6270, 16308.5713, 15523.7520, 15399.0898, 15461.5264, 15529.2881,\n",
      "        15249.3750, 15417.7793, 15964.7100, 16022.9365, 16179.8252, 15785.7148,\n",
      "        16097.7822, 16371.5088, 16060.6807, 15565.0967, 16213.9736, 15974.9707,\n",
      "        15895.4863, 15635.2588, 15689.6182, 15727.2402, 15491.9756, 15281.8408,\n",
      "        15355.0352, 15759.6758, 15273.2012, 15476.1182, 15526.8135, 16032.5225,\n",
      "        15797.0273, 15971.4600, 15911.3086, 15817.7324, 15251.4404, 15603.3232,\n",
      "        15664.0088, 15607.1123, 15527.6025, 15714.4639, 15661.4434, 15721.4102,\n",
      "        15507.7402, 15494.7793, 15546.4121, 15707.9639], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (\"error norm\", torch.norm(hwt_grad - wt_grad))\n",
    "#print(torch.max(torch.abs(hwt_grad - wt_grad)))\n",
    "#print(hwt_grad[hwt_grad != 0][:10])\n",
    "#print(wt_grad[hwt_grad != 0][:10])\n",
    "print(hwt_grad[wt_grad != 0])\n",
    "print(wt_grad[wt_grad != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-4\n",
    "_, f0 = myFunc(hashed_weight, input_v, random_numbers, input_dim, output_dim, chunk_size, TILED )\n",
    "int_grad = torch.empty_like(input_v)\n",
    "for i in range(int_grad.shape[0]):\n",
    "    for j in range(int_grad.shape[1]):\n",
    "        inputt = input_v.clone()\n",
    "        inputt[i][j] += epsilon\n",
    "        _, fi = myFunc(hashed_weight, inputt, random_numbers, input_dim, output_dim, chunk_size, TILED)\n",
    "        int_grad[i][j] = (fi - f0) / epsilon\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error norm tensor(213606.0938, device='cuda:0')\n",
      "tensor([[10000., 10000., 10000., 10000., 10000.],\n",
      "        [15000., 15000., 15000., 15000., 15000.],\n",
      "        [10000., 10000., 10000., 10000., 10000.],\n",
      "        [10000., 10000., 10000., 10000., 10000.],\n",
      "        [10000., 10000., 10000., 10000., 10000.]], device='cuda:0')\n",
      "tensor([[7991.0576, 8006.8306, 8022.6045, 7998.4473, 8014.2192],\n",
      "        [8395.5645, 8412.1270, 8428.6875, 8403.6836, 8420.2451],\n",
      "        [7688.5938, 7703.7388, 7718.8853, 7694.8872, 7710.0332],\n",
      "        [8171.5391, 8187.7100, 8203.8711, 8179.2729, 8195.4365],\n",
      "        [7735.8730, 7751.1753, 7766.4717, 7742.8701, 7758.1646]],\n",
      "       device='cuda:0')\n",
      "tensor(7592.6670, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (\"error norm\", torch.norm(int_grad - in_grad))\n",
    "print(int_grad[:5,:5])\n",
    "print(in_grad[:5,:5])\n",
    "print(torch.max(torch.abs(int_grad - in_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
